import asyncio
import json
import logging
import os
import re
import tempfile
from datetime import date, datetime, timedelta
from typing import Optional
from zoneinfo import ZoneInfo

from aiogram import Bot, F, Router
from aiogram.fsm.context import FSMContext
from aiogram.fsm.state import State, StatesGroup
from aiogram.types import CallbackQuery, Message
from aiogram.utils.keyboard import InlineKeyboardBuilder
from gspread.exceptions import WorksheetNotFound

from app.prompts import (
    DEFAULT_EXTRACT_USER,
    DEFAULT_MULTI_SYSTEM,
    DEFAULT_MULTI_USER,
    DEFAULT_ROUTER_USER,
    DEFAULT_THINKING_SYSTEM,
    DEFAULT_THINKING_USER,
    EXTRACT_PROMPT_KEY,
    ROUTER_PROMPT_KEY,
)
from app.handlers.delete import DeleteState, build_delete_keyboard, format_delete_list
from app.services.bot_settings_service import BotSettingsService
from app.services.delete_service import DeleteService
from app.services.intent_service import IntentService
from app.services.openai_service import OpenAIService
from app.services.qa_service import QAService
from app.services.router_service import RouterService
from app.services.sheets_service import SheetsService
from app.utils.auth import is_allowed, user_label

logger = logging.getLogger(__name__)

MAX_VOICE_SECONDS = 12 * 60
THINKING_MODE_SECONDS = 2 * 60
LONG_TRANSCRIPT_CHARS = 2500
MAX_TRANSCRIBE_TIMEOUT = 900
MAX_TG_CHARS = 3500


class IntakeState(StatesGroup):
    waiting_required = State()


class DuplicateState(StatesGroup):
    confirming = State()


class CategoryState(StatesGroup):
    selecting = State()


class ThinkingState(StatesGroup):
    waiting_choice = State()


def create_voice_router(
    openai_service: OpenAIService,
    sheets_service: SheetsService,
    router_service: RouterService,
    intent_service: IntentService,
    settings_service: BotSettingsService,
    qa_service: QAService,
    delete_service: DeleteService,
    allowed_user_ids: list[int],
    allowed_usernames: list[str],
) -> Router:
    router = Router()

    @router.message(F.voice)
    async def handle_voice(message: Message, bot: Bot, state: FSMContext) -> None:
        if not is_allowed(message.from_user, allowed_user_ids, allowed_usernames):
            logger.warning("Unauthorized user: %s", user_label(message.from_user))
            await message.answer("‚õîÔ∏è –î–æ—Å—Ç—É–ø –∑–∞–ø—Ä–µ—â–µ–Ω.")
            return

        logger.info("–ü–æ–ª—É—á–∏–ª –∞—É–¥–∏–æ")
        status_msg = await message.answer("‚è≥ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —Å–æ–æ–±—â–µ–Ω–∏–µ, —ç—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –¥–æ –º–∏–Ω—É—Ç—ã.")
        temp_path: Optional[str] = None
        transcript = ""
        category = ""
        today_str = datetime.now().strftime("%d.%m.%Y")
        today_date = datetime.now().date()

        try:
            if message.voice.duration > MAX_VOICE_SECONDS:
                await status_msg.edit_text(
                    "‚ö†Ô∏è –°–æ–æ–±—â–µ–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–µ. "
                    "–ú–∞–∫—Å–∏–º—É–º ‚Äî 12 –º–∏–Ω—É—Ç. "
                    "–†–∞–∑–±–µ–π—Ç–µ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ –≥–æ–ª–æ—Å–æ–≤—ã—Ö."
                )
                return

            if message.voice.duration > THINKING_MODE_SECONDS:
                minutes = max(1, round(message.voice.duration / 60))
                await status_msg.edit_text(
                    f"‚è≥ –î–ª–∏–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ ({minutes} –º–∏–Ω). "
                    "–≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç."
                )

            logger.info("–°–∫–∞—á–∏–≤–∞—é –∞—É–¥–∏–æ, –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å=%ss", message.voice.duration)
            step_start = asyncio.get_running_loop().time()
            temp_path = await asyncio.wait_for(_download_voice(bot, message), timeout=30)
            file_size = os.path.getsize(temp_path)
            logger.info("–ê—É–¥–∏–æ —Å–∫–∞—á–∞–Ω–æ –∑–∞ %.2fs, —Ä–∞–∑–º–µ—Ä=%s bytes", asyncio.get_running_loop().time() - step_start, file_size)

            logger.info("–û—Ç–ø—Ä–∞–≤–ª—è—é –≤ Whisper")
            step_start = asyncio.get_running_loop().time()
            transcribe_timeout = max(180, min(MAX_TRANSCRIBE_TIMEOUT, int(message.voice.duration * 3)))
            transcript = await asyncio.wait_for(openai_service.transcribe(temp_path), timeout=transcribe_timeout)
            if not transcript:
                raise ValueError("Empty transcription")
            logger.info("–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –≥–æ—Ç–æ–≤–∞ –∑–∞ %.2fs, —Å–∏–º–≤–æ–ª–æ–≤=%s", asyncio.get_running_loop().time() - step_start, len(transcript))

            logger.info("–ß–∏—Ç–∞—é –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –±–æ—Ç–∞")
            bot_settings = await settings_service.load()
            model = bot_settings.openai_model
            try:
                tz = ZoneInfo(bot_settings.timezone)
            except Exception:
                tz = ZoneInfo("UTC")
            now_tz = datetime.now(tz)
            today_str = now_tz.strftime("%d.%m.%Y")
            today_date = now_tz.date()

            if (
                message.voice.duration >= THINKING_MODE_SECONDS
                or len(transcript) >= LONG_TRANSCRIPT_CHARS
            ):
                await _handle_thinking_mode(
                    status_msg,
                    state,
                    openai_service,
                    transcript,
                    today_str,
                    model,
                )
                return

            logger.info("–û–ø—Ä–µ–¥–µ–ª—è—é –Ω–∞–º–µ—Ä–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è")
            intent = await intent_service.detect(transcript, model=model)
            action = intent.get("action", "add")
            query = intent.get("query", transcript)

            if action == "ask":
                logger.info("–†–µ–∂–∏–º –≤–æ–ø—Ä–æ—Å–∞")
                await status_msg.edit_text("‚è≥ –ò—â—É –ø–æ –±–∞–∑–µ, —ç—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –¥–æ –º–∏–Ω—É—Ç—ã.")
                answer = await qa_service.answer_question(query or transcript, model=model)
                await _send_long_text(status_msg, message, answer, safe_mode=bot_settings.safe_output)
                return

            if action == "delete":
                logger.info("–†–µ–∂–∏–º —É–¥–∞–ª–µ–Ω–∏—è")
                candidates = await delete_service.find_candidates(query or transcript, limit=7)
                if not candidates:
                    await status_msg.edit_text("‚ö†Ô∏è –ù–µ –Ω–∞—à–µ–ª –∑–∞–ø–∏—Å–µ–π –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è.")
                    return
                await state.set_state(DeleteState.selecting)
                await state.update_data(
                    candidates=[
                        {
                            "sheet_name": c.sheet_name,
                            "row_index": c.row_index,
                            "headers": c.headers,
                            "row_values": c.row_values,
                            "preview": c.preview,
                        }
                        for c in candidates
                    ]
                )
                kb = build_delete_keyboard(candidates)
                text = format_delete_list(candidates)
                await status_msg.edit_text(text, reply_markup=kb.as_markup())
                return

            logger.info("–ß–∏—Ç–∞—é Settings –∏–∑ Google Sheets")
            settings = await sheets_service.load_settings()
            logger.info("–ö–∞—Ç–µ–≥–æ—Ä–∏–π –Ω–∞–π–¥–µ–Ω–æ: %s", len(settings))

            logger.info("–ß–∏—Ç–∞—é Prompts –∏–∑ Google Sheets")
            prompts = await sheets_service.get_prompts()
            router_prompt = prompts.get(ROUTER_PROMPT_KEY, DEFAULT_ROUTER_USER)
            extract_prompt = prompts.get(EXTRACT_PROMPT_KEY, DEFAULT_EXTRACT_USER)

            multi_items = await _split_multi_items(
                openai_service,
                transcript,
                settings,
                model,
            )
            if len(multi_items) > 1:
                await _process_multi_items(
                    status_msg,
                    message,
                    state,
                    sheets_service,
                    router_service,
                    settings,
                    extract_prompt,
                    today_str,
                    multi_items,
                    model,
                    transcript,
                )
                return

            logger.info("–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é (model=%s)", model)
            try:
                category, _reasoning = await asyncio.wait_for(
                    router_service.classify_category(transcript, settings, router_prompt, model=model),
                    timeout=60,
                )
            except Exception:
                logger.exception("Failed to classify category")
                categories = list(settings.keys())
                await _prompt_category_choice(
                    status_msg,
                    state,
                    categories,
                    transcript,
                    today_str,
                )
                return
            logger.info("–û–ø—Ä–µ–¥–µ–ª–∏–ª –∫–∞—Ç–µ–≥–æ—Ä–∏—é: %s", category)

            logger.info("–ß–∏—Ç–∞—é –∑–∞–≥–æ–ª–æ–≤–∫–∏ –ª–∏—Å—Ç–∞: %s", category)
            headers = await sheets_service.get_headers(category)
            if not headers:
                raise ValueError("No headers found in target sheet")
            logger.info("–ù–∞—à–µ–ª —Å—Ç–æ–ª–±—Ü—ã: %s", headers)

            logger.info("–ò–∑–≤–ª–µ–∫–∞—é –¥–∞–Ω–Ω—ã–µ –ø–æ–¥ –∑–∞–≥–æ–ª–æ–≤–∫–∏ (model=%s)", model)
            clean_headers = [_clean_header(header) for header in headers]
            row = await asyncio.wait_for(
                router_service.extract_row(transcript, clean_headers, today_str, extract_prompt, model=model),
                timeout=60,
            )
            row = _apply_text_fields(headers, row, transcript)
            row = _apply_date_fields(headers, row, transcript, today_date)

            missing_required = _get_missing_required(headers, row)
            if missing_required:
                await state.set_state(IntakeState.waiting_required)
                await state.update_data(
                    category=category,
                    headers=headers,
                    row=row,
                    transcript=transcript,
                    today_str=today_str,
                    missing_required_indices=[idx for idx, _name in missing_required],
                )
                if len(missing_required) == 1 and _is_priority_header(missing_required[0][1]):
                    kb = _build_priority_keyboard()
                    await status_msg.edit_text(
                        "‚ö†Ô∏è –ù—É–∂–Ω–æ –≤—ã–±—Ä–∞—Ç—å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –∑–∞–¥–∞—á–∏:",
                        reply_markup=kb.as_markup(),
                    )
                    return

                missing_names = ", ".join(name for _idx, name in missing_required)
                await status_msg.edit_text(
                    "‚ö†Ô∏è –ù—É–∂–Ω–æ –∑–∞–ø–æ–ª–Ω–∏—Ç—å –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è:\n"
                    f"{missing_names}\n\n"
                    "–ù–∞–ø–∏—à–∏—Ç–µ –æ—Ç–≤–µ—Ç —Ç–∞–∫:\n"
                    "–ü–æ–ª–µ=–∑–Ω–∞—á–µ–Ω–∏–µ; –ü–æ–ª–µ=–∑–Ω–∞—á–µ–Ω–∏–µ\n"
                    "–ü—Ä–∏–º–µ—Ä: –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç=–í—ã—Å–æ–∫–∏–π\n\n"
                    "–ß—Ç–æ–±—ã –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å ‚Äî –Ω–∞–∂–º–∏—Ç–µ ¬´–ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å¬ª –∏–ª–∏ —Å–∫–∞–∂–∏—Ç–µ ¬´off¬ª.\n"
                    "–ß—Ç–æ–±—ã –æ—Ç–º–µ–Ω–∏—Ç—å ‚Äî –Ω–∞–ø–∏—à–∏—Ç–µ ¬´–û—Ç–º–µ–Ω–∞¬ª.",
                    reply_markup=_build_required_keyboard().as_markup(),
                )
                return

            duplicate_preview = await _find_duplicate(sheets_service, category, headers, row)
            if duplicate_preview:
                await state.set_state(DuplicateState.confirming)
                await state.update_data(
                    category=category,
                    headers=headers,
                    row=row,
                    transcript=transcript,
                    today_str=today_str,
                    duplicate_preview=duplicate_preview,
                )
                await status_msg.edit_text(
                    "‚ö†Ô∏è –ü–æ—Ö–æ–∂–µ, —ç—Ç–æ –¥—É–±–ª–∏–∫–∞—Ç.\n\n"
                    f"{duplicate_preview}\n\n"
                    "–î–æ–±–∞–≤–∏—Ç—å –Ω–æ–≤—É—é –∑–∞–ø–∏—Å—å?",
                    reply_markup=_build_duplicate_keyboard().as_markup(),
                )
                return

            logger.info("–ó–∞–ø–∏—Å—ã–≤–∞—é —Å—Ç—Ä–æ–∫—É –≤ –ª–∏—Å—Ç: %s", category)
            await sheets_service.append_row(category, row)
            logger.info("–ü–∏—à—É –≤ Inbox")
            await sheets_service.append_row("Inbox", [today_str, category, transcript])
            logger.info("–ó–∞–ø–∏—Å–∞–ª —Å—Ç—Ä–æ–∫—É")

            short_text = transcript if len(transcript) <= 300 else transcript[:297] + "..."
            short_text = _get_summary_value(headers, row) or short_text
            await status_msg.edit_text(
                f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ '{category}'.\n"
                f"–°—É—Ç—å: {short_text}\n"
                f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è: {category}"
            )
        except json.JSONDecodeError:
            logger.exception("GPT returned invalid JSON")
            await status_msg.edit_text("‚ö†Ô∏è GPT –≤–µ—Ä–Ω—É–ª –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π JSON. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑.")
            await _safe_inbox(sheets_service, today_str, category or "Unknown", transcript)
        except asyncio.TimeoutError:
            logger.exception("Timeout while processing message")
            await status_msg.edit_text("‚ö†Ô∏è –ü—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç –ò–ò. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑.")
            await _safe_inbox(sheets_service, today_str, category or "Unknown", transcript)
        except WorksheetNotFound:
            logger.exception("Worksheet not found")
            await status_msg.edit_text("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω –ª–∏—Å—Ç –≤ Google Sheets. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏.")
            await _safe_inbox(sheets_service, today_str, category or "Unknown", transcript)
        except Exception:
            logger.exception("Unhandled error")
            await status_msg.edit_text("‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏—è. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑.")
            await _safe_inbox(sheets_service, today_str, category or "Unknown", transcript)
        finally:
            if temp_path:
                try:
                    os.remove(temp_path)
                except OSError:
                    logger.warning("Failed to remove temp file: %s", temp_path)

    @router.callback_query(IntakeState.waiting_required, F.data == "req:cancel")
    async def cancel_required(callback: CallbackQuery, state: FSMContext) -> None:
        if not is_allowed(callback.from_user, allowed_user_ids, allowed_usernames):
            await callback.answer("–î–æ—Å—Ç—É–ø –∑–∞–ø—Ä–µ—â–µ–Ω", show_alert=True)
            return
        await state.clear()
        await callback.message.edit_text("–û–∫, –æ—Ç–º–µ–Ω–∏–ª.")
        await callback.answer()

    @router.callback_query(IntakeState.waiting_required, F.data == "req:skip")
    async def skip_required(callback: CallbackQuery, state: FSMContext) -> None:
        if not is_allowed(callback.from_user, allowed_user_ids, allowed_usernames):
            await callback.answer("–î–æ—Å—Ç—É–ø –∑–∞–ø—Ä–µ—â–µ–Ω", show_alert=True)
            return

        data = await state.get_data()
        pending_info = _get_pending_item(data)
        if pending_info:
            pending, _idx, item = pending_info
            results = data.get("multi_results", [])
            missing_indices = data.get("missing_required_indices", [])
            row = item.get("row", [])
            if missing_indices:
                for idx in missing_indices:
                    if isinstance(idx, int) and idx < len(row):
                        row[idx] = ""
                item["row"] = row
                pending[_idx] = item
                await state.update_data(pending_items=pending)
            await _finalize_multi_item(callback.message, callback.message, state, sheets_service, item, results)
            await callback.answer()
            return

        category = data.get("category", "")
        headers = data.get("headers", [])
        row = data.get("row", [])
        transcript = data.get("transcript", "")
        today_str = data.get("today_str", datetime.now().strftime("%d.%m.%Y"))
        missing_indices = data.get("missing_required_indices", [])
        today_date = _parse_date_value(today_str) or datetime.now().date()
        today_date = _parse_date_value(today_str) or datetime.now().date()
        today_date = _parse_date_value(today_str) or datetime.now().date()
        today_date = _parse_date_value(today_str) or datetime.now().date()

        if not category or not headers or not row:
            await state.clear()
            await callback.message.edit_text("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç. –ü–æ–≤—Ç–æ—Ä–∏—Ç–µ –∑–∞–ø–∏—Å—å.")
            await callback.answer()
            return

        if missing_indices:
            for idx in missing_indices:
                if isinstance(idx, int) and idx < len(row):
                    row[idx] = ""
            await state.update_data(row=row)

        duplicate_preview = await _find_duplicate(sheets_service, category, headers, row)
        if duplicate_preview:
            await state.set_state(DuplicateState.confirming)
            await state.update_data(
                category=category,
                headers=headers,
                row=row,
                transcript=transcript,
                today_str=today_str,
                duplicate_preview=duplicate_preview,
            )
            await callback.message.edit_text(
                "‚ö†Ô∏è –ü–æ—Ö–æ–∂–µ, —ç—Ç–æ –¥—É–±–ª–∏–∫–∞—Ç.\n\n"
                f"{duplicate_preview}\n\n"
                "–î–æ–±–∞–≤–∏—Ç—å –Ω–æ–≤—É—é –∑–∞–ø–∏—Å—å?",
                reply_markup=_build_duplicate_keyboard().as_markup(),
            )
            await callback.answer()
            return

        row = _apply_text_fields(headers, row, transcript)
        row = _apply_date_fields(headers, row, transcript, today_date)
        await sheets_service.append_row(category, row)
        await sheets_service.append_row("Inbox", [today_str, category, transcript])
        await state.clear()
        short_text = transcript if len(transcript) <= 300 else transcript[:297] + "..."
        short_text = _get_summary_value(headers, row) or short_text
        await callback.message.edit_text(
            f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ '{category}' –±–µ–∑ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π.\n"
            f"–°—É—Ç—å: {short_text}\n"
            f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è: {category}"
        )
        await callback.answer()

    @router.callback_query(IntakeState.waiting_required, F.data.startswith("req:priority:"))
    async def handle_required_priority(callback: CallbackQuery, state: FSMContext) -> None:
        if not is_allowed(callback.from_user, allowed_user_ids, allowed_usernames):
            await callback.answer("–î–æ—Å—Ç—É–ø –∑–∞–ø—Ä–µ—â–µ–Ω", show_alert=True)
            return
        value_map = {
            "low": "–ù–∏–∑–∫–∏–π",
            "medium": "–°—Ä–µ–¥–Ω–∏–π",
            "high": "–í—ã—Å–æ–∫–∏–π",
        }
        code = callback.data.split(":")[-1]
        value = value_map.get(code)
        if not value:
            await callback.answer("–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç", show_alert=True)
            return

        data = await state.get_data()
        pending_info = _get_pending_item(data)
        if pending_info:
            pending, idx, item = pending_info
            headers = item.get("headers", [])
            row = item.get("row", [])
            transcript = item.get("transcript", "")
            today_str = item.get("today_str", datetime.now().strftime("%d.%m.%Y"))
            today_date = _parse_date_value(today_str) or datetime.now().date()
            if len(row) < len(headers):
                row.extend([""] * (len(headers) - len(row)))

            item_idx = None
            for i, header in enumerate(headers):
                if _is_priority_header(_display_header(header)):
                    item_idx = i
                    break
            if item_idx is None:
                await callback.message.edit_text("‚ö†Ô∏è –ü–æ–ª–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
                await callback.answer()
                return
            if item_idx < len(row):
                row[item_idx] = value

            item["row"] = row
            pending[idx] = item
            await state.update_data(pending_items=pending)

            missing_after = _get_missing_required(headers, row)
            if missing_after:
                await _prompt_required_item(callback.message, state)
                await callback.answer()
                return

            results = data.get("multi_results", [])
            await _finalize_multi_item(callback.message, callback.message, state, sheets_service, item, results)
            await callback.answer()
            return
        category = data.get("category", "")
        headers = data.get("headers", [])
        row = data.get("row", [])
        transcript = data.get("transcript", "")
        today_str = data.get("today_str", datetime.now().strftime("%d.%m.%Y"))
        today_date = _parse_date_value(today_str) or datetime.now().date()
        if len(row) < len(headers):
            row.extend([""] * (len(headers) - len(row)))

        if not category or not headers or not row:
            await state.clear()
            await callback.message.edit_text("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç. –ü–æ–≤—Ç–æ—Ä–∏—Ç–µ –∑–∞–ø–∏—Å—å.")
            await callback.answer()
            return

        idx = None
        for i, header in enumerate(headers):
            if _is_priority_header(_display_header(header)):
                idx = i
                break
        if idx is None:
            await callback.message.edit_text("‚ö†Ô∏è –ü–æ–ª–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
            await callback.answer()
            return

        if idx < len(row):
            row[idx] = value

        missing_after = _get_missing_required(headers, row)
        if missing_after:
            missing_names = ", ".join(name for _idx, name in missing_after)
            await state.update_data(row=row)
            await callback.message.edit_text(
                "‚ö†Ô∏è –ù—É–∂–Ω–æ –∑–∞–ø–æ–ª–Ω–∏—Ç—å –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è:\n"
                f"{missing_names}\n\n"
                "–ù–∞–ø–∏—à–∏—Ç–µ –æ—Ç–≤–µ—Ç —Ç–∞–∫:\n"
                "–ü–æ–ª–µ=–∑–Ω–∞—á–µ–Ω–∏–µ; –ü–æ–ª–µ=–∑–Ω–∞—á–µ–Ω–∏–µ\n"
                "–ü—Ä–∏–º–µ—Ä: –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç=–í—ã—Å–æ–∫–∏–π\n"
                "–ú–æ–∂–Ω–æ –Ω–∞–∂–∞—Ç—å ¬´–ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å¬ª.",
                reply_markup=_build_required_keyboard().as_markup(),
            )
            await callback.answer()
            return

        duplicate_preview = await _find_duplicate(sheets_service, category, headers, row)
        if duplicate_preview:
            await state.set_state(DuplicateState.confirming)
            await state.update_data(
                category=category,
                headers=headers,
                row=row,
                transcript=transcript,
                today_str=today_str,
                duplicate_preview=duplicate_preview,
            )
            await callback.message.edit_text(
                "‚ö†Ô∏è –ü–æ—Ö–æ–∂–µ, —ç—Ç–æ –¥—É–±–ª–∏–∫–∞—Ç.\n\n"
                f"{duplicate_preview}\n\n"
                "–î–æ–±–∞–≤–∏—Ç—å –Ω–æ–≤—É—é –∑–∞–ø–∏—Å—å?",
                reply_markup=_build_duplicate_keyboard().as_markup(),
            )
            await callback.answer()
            return

        row = _apply_text_fields(headers, row, transcript)
        row = _apply_date_fields(headers, row, transcript, today_date)
        await sheets_service.append_row(category, row)
        await sheets_service.append_row("Inbox", [today_str, category, transcript])
        await state.clear()
        short_text = transcript if len(transcript) <= 300 else transcript[:297] + "..."
        short_text = _get_summary_value(headers, row) or short_text
        await callback.message.edit_text(
            f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ '{category}'.\n"
            f"–°—É—Ç—å: {short_text}\n"
            f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è: {category}"
        )
        await callback.answer()

    @router.callback_query(DuplicateState.confirming, F.data == "dup:add")
    async def confirm_duplicate_add(callback: CallbackQuery, state: FSMContext) -> None:
        if not is_allowed(callback.from_user, allowed_user_ids, allowed_usernames):
            await callback.answer("–î–æ—Å—Ç—É–ø –∑–∞–ø—Ä–µ—â–µ–Ω", show_alert=True)
            return
        data = await state.get_data()
        pending_info = _get_pending_item(data)
        if pending_info:
            _pending, _idx, item = pending_info
            category = item.get("category", "")
            headers = item.get("headers", [])
            row = item.get("row", [])
            transcript = item.get("transcript", "")
            today_str = item.get("today_str", datetime.now().strftime("%d.%m.%Y"))
        else:
            category = data.get("category", "")
            headers = data.get("headers", [])
            row = data.get("row", [])
            transcript = data.get("transcript", "")
            today_str = data.get("today_str", datetime.now().strftime("%d.%m.%Y"))
        today_date = _parse_date_value(today_str) or datetime.now().date()

        if not category or not headers or not row:
            await state.clear()
            await callback.message.edit_text("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç. –ü–æ–≤—Ç–æ—Ä–∏—Ç–µ –∑–∞–ø–∏—Å—å.")
            await callback.answer()
            return

        row = _apply_text_fields(headers, row, transcript)
        row = _apply_date_fields(headers, row, transcript, today_date)
        await sheets_service.append_row(category, row)
        await sheets_service.append_row("Inbox", [today_str, category, transcript])
        await state.clear()
        short_text = transcript if len(transcript) <= 300 else transcript[:297] + "..."
        short_text = _get_summary_value(headers, row) or short_text
        await callback.message.edit_text(
            f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ –∫–∞–∫ –Ω–æ–≤–∞—è –∑–∞–ø–∏—Å—å –≤ '{category}'.\n"
            f"–°—É—Ç—å: {short_text}\n"
            f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è: {category}"
        )
        await callback.answer()

    @router.callback_query(DuplicateState.confirming, F.data == "dup:skip")
    async def confirm_duplicate_skip(callback: CallbackQuery, state: FSMContext) -> None:
        if not is_allowed(callback.from_user, allowed_user_ids, allowed_usernames):
            await callback.answer("–î–æ—Å—Ç—É–ø –∑–∞–ø—Ä–µ—â–µ–Ω", show_alert=True)
            return
        await state.clear()
        await callback.message.edit_text("–û–∫, –Ω–µ –¥–æ–±–∞–≤–ª—è—é –¥—É–±–ª–∏–∫–∞—Ç.")
        await callback.answer()

    @router.callback_query(CategoryState.selecting, F.data == "cat:cancel")
    async def cancel_category_pick(callback: CallbackQuery, state: FSMContext) -> None:
        if not is_allowed(callback.from_user, allowed_user_ids, allowed_usernames):
            await callback.answer("–î–æ—Å—Ç—É–ø –∑–∞–ø—Ä–µ—â–µ–Ω", show_alert=True)
            return
        await state.clear()
        await callback.message.edit_text("–û–∫, –æ—Ç–º–µ–Ω–∏–ª.")
        await callback.answer()

    @router.callback_query(CategoryState.selecting, F.data.startswith("cat:pick:"))
    async def pick_category(callback: CallbackQuery, state: FSMContext) -> None:
        if not is_allowed(callback.from_user, allowed_user_ids, allowed_usernames):
            await callback.answer("–î–æ—Å—Ç—É–ø –∑–∞–ø—Ä–µ—â–µ–Ω", show_alert=True)
            return

        data = await state.get_data()
        categories = data.get("categories", [])
        transcript = data.get("transcript", "")
        today_str = data.get("today_str", datetime.now().strftime("%d.%m.%Y"))

        try:
            index = int(callback.data.split(":")[-1])
        except ValueError:
            await callback.answer("–û—à–∏–±–∫–∞ –≤—ã–±–æ—Ä–∞", show_alert=True)
            return

        if index < 0 or index >= len(categories):
            await callback.answer("–ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä", show_alert=True)
            return

        category = categories[index]

        try:
            headers = await sheets_service.get_headers(category)
            if not headers:
                await callback.message.edit_text("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω —Å–ø–∏—Å–æ–∫ —Å—Ç–æ–ª–±—Ü–æ–≤ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏.")
                await state.clear()
                await callback.answer()
                return

            prompts = await sheets_service.get_prompts()
            extract_prompt = prompts.get(EXTRACT_PROMPT_KEY, DEFAULT_EXTRACT_USER)

            bot_settings = await settings_service.load()
            model = bot_settings.openai_model

            clean_headers = [_clean_header(header) for header in headers]
            row = await asyncio.wait_for(
                router_service.extract_row(transcript, clean_headers, today_str, extract_prompt, model=model),
                timeout=60,
            )
            row = _apply_text_fields(headers, row, transcript)
            row = _apply_date_fields(headers, row, transcript, _parse_date_value(today_str) or datetime.now().date())
            row = _apply_date_fields(headers, row, transcript, today_date)

            missing_required = _get_missing_required(headers, row)
            if missing_required:
                await state.set_state(IntakeState.waiting_required)
                await state.update_data(
                    category=category,
                    headers=headers,
                    row=row,
                    transcript=transcript,
                    today_str=today_str,
                    missing_required_indices=[idx for idx, _name in missing_required],
                )
                if len(missing_required) == 1 and _is_priority_header(missing_required[0][1]):
                    kb = _build_priority_keyboard()
                    await callback.message.edit_text(
                        "‚ö†Ô∏è –ù—É–∂–Ω–æ –≤—ã–±—Ä–∞—Ç—å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –∑–∞–¥–∞—á–∏:",
                        reply_markup=kb.as_markup(),
                    )
                    await callback.answer()
                    return

                missing_names = ", ".join(name for _idx, name in missing_required)
                await callback.message.edit_text(
                    "‚ö†Ô∏è –ù—É–∂–Ω–æ –∑–∞–ø–æ–ª–Ω–∏—Ç—å –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è:\n"
                    f"{missing_names}\n\n"
                    "–ù–∞–ø–∏—à–∏—Ç–µ –æ—Ç–≤–µ—Ç —Ç–∞–∫:\n"
                    "–ü–æ–ª–µ=–∑–Ω–∞—á–µ–Ω–∏–µ; –ü–æ–ª–µ=–∑–Ω–∞—á–µ–Ω–∏–µ\n"
                    "–ü—Ä–∏–º–µ—Ä: –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç=–í—ã—Å–æ–∫–∏–π\n\n"
                    "–ß—Ç–æ–±—ã –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å ‚Äî –Ω–∞–∂–º–∏—Ç–µ ¬´–ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å¬ª.\n"
                    "–ß—Ç–æ–±—ã –æ—Ç–º–µ–Ω–∏—Ç—å ‚Äî –Ω–∞–ø–∏—à–∏—Ç–µ ¬´–û—Ç–º–µ–Ω–∞¬ª.",
                    reply_markup=_build_required_keyboard().as_markup(),
                )
                await callback.answer()
                return

            duplicate_preview = await _find_duplicate(sheets_service, category, headers, row)
            if duplicate_preview:
                await state.set_state(DuplicateState.confirming)
                await state.update_data(
                    category=category,
                    headers=headers,
                    row=row,
                    transcript=transcript,
                    today_str=today_str,
                    duplicate_preview=duplicate_preview,
                )
                await callback.message.edit_text(
                    "‚ö†Ô∏è –ü–æ—Ö–æ–∂–µ, —ç—Ç–æ –¥—É–±–ª–∏–∫–∞—Ç.\n\n"
                    f"{duplicate_preview}\n\n"
                    "–î–æ–±–∞–≤–∏—Ç—å –Ω–æ–≤—É—é –∑–∞–ø–∏—Å—å?",
                    reply_markup=_build_duplicate_keyboard().as_markup(),
                )
                await callback.answer()
                return

            await sheets_service.append_row(category, row)
            await sheets_service.append_row("Inbox", [today_str, category, transcript])
            await state.clear()
            short_text = transcript if len(transcript) <= 300 else transcript[:297] + "..."
            short_text = _get_summary_value(headers, row) or short_text
            await callback.message.edit_text(
                f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ '{category}'.\n"
                f"–°—É—Ç—å: {short_text}\n"
                f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è: {category}"
            )
            await callback.answer()
        except Exception:
            logger.exception("Failed to process category selection")
            await callback.message.edit_text("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤—ã–±–æ—Ä –∫–∞—Ç–µ–≥–æ—Ä–∏–∏. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â—ë —Ä–∞–∑.")
            await state.clear()
            await callback.answer()

    @router.message(IntakeState.waiting_required, F.text)
    async def handle_required_fields(message: Message, state: FSMContext) -> None:
        if not is_allowed(message.from_user, allowed_user_ids, allowed_usernames):
            await message.answer("‚õîÔ∏è –î–æ—Å—Ç—É–ø –∑–∞–ø—Ä–µ—â–µ–Ω.")
            return

        text = message.text.strip()
        if text.lower() in {"–æ—Ç–º–µ–Ω–∞", "cancel", "—Å—Ç–æ–ø"}:
            await state.clear()
            await message.answer("–û–∫, –æ—Ç–º–µ–Ω–∏–ª.")
            return

        data = await state.get_data()
        category = data.get("category", "")
        headers = data.get("headers", [])
        row = data.get("row", [])
        transcript = data.get("transcript", "")
        today_str = data.get("today_str", datetime.now().strftime("%d.%m.%Y"))

        if not category or not headers or not row:
            await state.clear()
            await message.answer("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç. –ü–æ–≤—Ç–æ—Ä–∏—Ç–µ –∑–∞–ø–∏—Å—å.")
            return

        if text.lower() in {"off", "–ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å", "skip"}:
            row = _apply_text_fields(headers, row, transcript)
            row = _apply_date_fields(headers, row, transcript, today_date)
            await sheets_service.append_row(category, row)
            await sheets_service.append_row("Inbox", [today_str, category, transcript])
            await state.clear()
            short_text = transcript if len(transcript) <= 300 else transcript[:297] + "..."
            short_text = _get_summary_value(headers, row) or short_text
            await message.answer(
                f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ '{category}' –±–µ–∑ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π.\n"
                f"–°—É—Ç—å: {short_text}\n"
                f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è: {category}"
            )
            return

        required = _get_missing_required(headers, row)
        required_map = {name.lower(): idx for idx, name in required}

        updates = _parse_key_values(text, required_map)
        if not updates and len(required) == 1:
            idx, _name = required[0]
            row[idx] = text
        else:
            for idx, value in updates.items():
                if idx < len(row):
                    row[idx] = value

        missing_after = _get_missing_required(headers, row)
        if missing_after:
            missing_names = ", ".join(name for _idx, name in missing_after)
            await message.answer(
                "‚ö†Ô∏è –ù—É–∂–Ω–æ –∑–∞–ø–æ–ª–Ω–∏—Ç—å –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è:\n"
                f"{missing_names}\n\n"
                "–ù–∞–ø–∏—à–∏—Ç–µ –æ—Ç–≤–µ—Ç —Ç–∞–∫:\n"
                "–ü–æ–ª–µ=–∑–Ω–∞—á–µ–Ω–∏–µ; –ü–æ–ª–µ=–∑–Ω–∞—á–µ–Ω–∏–µ\n"
                "–ü—Ä–∏–º–µ—Ä: –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç=–í—ã—Å–æ–∫–∏–π\n"
                "–ú–æ–∂–Ω–æ –Ω–∞–ø–∏—Å–∞—Ç—å ¬´off¬ª –∏–ª–∏ ¬´–ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å¬ª."
            )
            await state.update_data(row=row)
            return

        duplicate_preview = await _find_duplicate(sheets_service, category, headers, row)
        if duplicate_preview:
            await state.set_state(DuplicateState.confirming)
            await state.update_data(
                category=category,
                headers=headers,
                row=row,
                transcript=transcript,
                today_str=today_str,
                duplicate_preview=duplicate_preview,
            )
            await message.answer(
                "‚ö†Ô∏è –ü–æ—Ö–æ–∂–µ, —ç—Ç–æ –¥—É–±–ª–∏–∫–∞—Ç.\n\n"
                f"{duplicate_preview}\n\n"
                "–î–æ–±–∞–≤–∏—Ç—å –Ω–æ–≤—É—é –∑–∞–ø–∏—Å—å?",
                reply_markup=_build_duplicate_keyboard().as_markup(),
            )
            return

        row = _apply_text_fields(headers, row, transcript)
        row = _apply_date_fields(headers, row, transcript, today_date)
        await sheets_service.append_row(category, row)
        await sheets_service.append_row("Inbox", [today_str, category, transcript])
        await state.clear()

        short_text = transcript if len(transcript) <= 300 else transcript[:297] + "..."
        short_text = _get_summary_value(headers, row) or short_text
        await message.answer(
            f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ '{category}'.\n"
            f"–°—É—Ç—å: {short_text}\n"
            f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è: {category}"
        )

    @router.callback_query(ThinkingState.waiting_choice, F.data.startswith("thinking:"))
    async def handle_thinking_choice(callback: CallbackQuery, state: FSMContext) -> None:
        if not is_allowed(callback.from_user, allowed_user_ids, allowed_usernames):
            await callback.answer("–î–æ—Å—Ç—É–ø –∑–∞–ø—Ä–µ—â–µ–Ω", show_alert=True)
            return
        await callback.answer()
        data = await state.get_data()
        structured = data.get("thinking_structured") or {}
        transcript = data.get("thinking_transcript") or ""
        today_str = data.get("thinking_today_str") or datetime.now().strftime("%d.%m.%Y")

        action = callback.data.split(":", 1)[1]
        if action == "cancel":
            await state.clear()
            await callback.message.edit_text("–û–∫, –Ω–∏—á–µ–≥–æ –Ω–µ —Å–æ—Ö—Ä–∞–Ω—è—é.")
            return
        if action != "save":
            await callback.message.edit_text("‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ.")
            return

        save_text = _build_thinking_inbox_text(structured, transcript)
        try:
            await sheets_service.append_row("–ü—Ä–æ—á–µ–µ", [today_str, "Thinking", save_text])
            await state.clear()
            await callback.message.edit_text("‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ ¬´–ü—Ä–æ—á–µ–µ¬ª.")
        except WorksheetNotFound:
            await sheets_service.append_row("Inbox", [today_str, "Thinking", save_text])
            await state.clear()
            await callback.message.edit_text("‚ö†Ô∏è –õ–∏—Å—Ç ¬´–ü—Ä–æ—á–µ–µ¬ª –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–æ—Ö—Ä–∞–Ω–∏–ª –≤ Inbox.")

    return router


async def _download_voice(bot: Bot, message: Message) -> str:
    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".ogg")
    temp_path = temp_file.name
    temp_file.close()

    file = await bot.get_file(message.voice.file_id)
    await bot.download_file(file.file_path, destination=temp_path)
    return temp_path


async def _safe_inbox(
    sheets_service: SheetsService,
    today_str: str,
    category: str,
    transcript: str,
) -> None:
    if not transcript:
        return
    try:
        await sheets_service.append_row("Inbox", [today_str, category, transcript])
    except Exception:
        logger.exception("Failed to write to Inbox")


def _get_missing_required(headers: list[str], row: list[str]) -> list[tuple[int, str]]:
    missing = []
    for idx, header in enumerate(headers):
        if header.strip().endswith("*"):
            value = row[idx] if idx < len(row) else ""
            if not str(value).strip():
                missing.append((idx, _display_header(header)))
    return missing


def _display_header(header: str) -> str:
    return header.replace("*", "").strip()


def _clean_header(header: str) -> str:
    return header.replace("*", "").strip()


def _parse_key_values(text: str, header_map: dict[str, int]) -> dict[int, str]:
    result: dict[int, str] = {}
    parts = [part.strip() for part in text.split(";") if part.strip()]
    lines = []
    for part in parts:
        lines.extend([line.strip() for line in part.split("\n") if line.strip()])

    for line in lines:
        if ":" in line:
            key, value = line.split(":", 1)
        elif "=" in line:
            key, value = line.split("=", 1)
        elif " - " in line:
            key, value = line.split(" - ", 1)
        else:
            continue
        key_norm = _display_header(key).lower()
        if key_norm in header_map:
            result[header_map[key_norm]] = value.strip()

    return result


def _apply_text_fields(headers: list[str], row: list[str], transcript: str) -> list[str]:
    raw_idx = _find_header_index(headers, {"—Å—ã—Ä–æ–π —Ç–µ–∫—Å—Ç", "raw text", "original text", "–∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç"})
    summary_idx = _find_header_index(headers, {"—Å—É—Ç—å", "–æ–ø–∏—Å–∞–Ω–∏–µ", "summary"})

    if raw_idx is not None and raw_idx < len(row):
        row[raw_idx] = transcript

    if summary_idx is not None and summary_idx < len(row):
        summary_value = str(row[summary_idx]).strip()
        raw_value = transcript.strip()
        raw_col_value = ""
        if raw_idx is not None and raw_idx < len(row):
            raw_col_value = str(row[raw_idx]).strip()

        if (
            not summary_value
            or _normalize_text(summary_value) == _normalize_text(raw_value)
            or _normalize_text(summary_value) == _normalize_text(raw_col_value)
        ):
            row[summary_idx] = _make_summary(transcript)

    return row


def _apply_date_fields(headers: list[str], row: list[str], transcript: str, today: date) -> list[str]:
    date_explicit = _extract_explicit_date(transcript)
    date_relative = _extract_relative_date(transcript, today)
    target_date = date_explicit or date_relative

    today_str = today.strftime("%d.%m.%Y")
    for idx, header in enumerate(headers):
        key = _display_header(header).lower().strip()
        if key == "–¥–∞—Ç–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è":
            if idx < len(row):
                row[idx] = today_str

    if target_date:
        target_str = target_date.strftime("%d.%m.%Y")
        for idx, header in enumerate(headers):
            key = _display_header(header).lower().strip()
            if key in {"–¥–∞—Ç–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è", "–¥–∞—Ç–∞", "date", "due date"}:
                if idx < len(row):
                    row[idx] = target_str
    return row


def _extract_explicit_date(text: str) -> date | None:
    match = re.search(r"(\d{2}\.\d{2}\.\d{4})", text)
    if match:
        return _parse_date_value(match.group(1))
    match = re.search(r"(\d{4}-\d{2}-\d{2})", text)
    if match:
        return _parse_date_value(match.group(1))
    return None


def _extract_relative_date(text: str, today: date) -> date | None:
    lowered = text.lower()
    if "–ø–æ—Å–ª–µ–∑–∞–≤—Ç—Ä–∞" in lowered:
        return today + timedelta(days=2)
    if "–∑–∞–≤—Ç—Ä–∞" in lowered:
        return today + timedelta(days=1)
    if "—Å–µ–≥–æ–¥–Ω—è" in lowered:
        return today

    weekday = _find_weekday(lowered)
    if weekday is None:
        return None

    if "—Å–ª–µ–¥—É—é—â" in lowered:
        days_to_next_monday = (7 - today.weekday()) or 7
        next_monday = today + timedelta(days=days_to_next_monday)
        return next_monday + timedelta(days=weekday)

    delta = (weekday - today.weekday()) % 7
    return today + timedelta(days=delta)


def _find_weekday(text: str) -> int | None:
    mapping = {
        "–ø–æ–Ω–µ–¥": 0,
        "–≤—Ç–æ—Ä": 1,
        "—Å—Ä–µ–¥": 2,
        "—á–µ—Ç–≤–µ—Ä": 3,
        "—á–µ—Ç–≤–µ—Ä–≥": 3,
        "–ø—è—Ç–Ω–∏—Ü": 4,
        "—Å—É–±–±–æ—Ç": 5,
        "–≤–æ—Å–∫—Ä–µ—Å": 6,
    }
    for key, idx in mapping.items():
        if key in text:
            return idx
    return None


def _parse_date_value(value: str) -> date | None:
    value = value.strip()
    if not value:
        return None
    for fmt in ("%d.%m.%Y", "%Y-%m-%d"):
        try:
            return datetime.strptime(value, fmt).date()
        except ValueError:
            continue
    return None


def _get_summary_value(headers: list[str], row: list[str]) -> str:
    idx = _find_header_index(headers, {"—Å—É—Ç—å", "–æ–ø–∏—Å–∞–Ω–∏–µ", "summary"})
    if idx is None or idx >= len(row):
        return ""
    return str(row[idx]).strip()


async def _find_duplicate(
    sheets_service: SheetsService,
    category: str,
    headers: list[str],
    row: list[str],
    limit: int = 50,
) -> str | None:
    try:
        rows = await sheets_service.get_all_values(category)
    except Exception:
        logger.exception("Failed to read sheet for duplicate check: %s", category)
        return None

    if not rows or len(rows) < 2:
        return None

    header_row = rows[0]
    summary_new = _get_value_by_headers(headers, row, {"—Å—É—Ç—å", "–æ–ø–∏—Å–∞–Ω–∏–µ", "summary"})
    raw_new = _get_value_by_headers(headers, row, {"—Å—ã—Ä–æ–π —Ç–µ–∫—Å—Ç", "raw text", "original text", "–∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç"})
    date_new = _get_value_by_headers(headers, row, {"–¥–∞—Ç–∞", "–¥–∞—Ç–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è", "–¥–∞—Ç–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è", "date"})

    recent_rows = rows[1:][-limit:]
    for old in reversed(recent_rows):
        summary_old = _get_value_by_headers(header_row, old, {"—Å—É—Ç—å", "–æ–ø–∏—Å–∞–Ω–∏–µ", "summary"})
        raw_old = _get_value_by_headers(header_row, old, {"—Å—ã—Ä–æ–π —Ç–µ–∫—Å—Ç", "raw text", "original text", "–∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç"})
        date_old = _get_value_by_headers(header_row, old, {"–¥–∞—Ç–∞", "–¥–∞—Ç–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è", "–¥–∞—Ç–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è", "date"})

        if _is_duplicate(summary_new, raw_new, date_new, summary_old, raw_old, date_old):
            return _format_duplicate_preview(header_row, old)

    return None


def _is_duplicate(
    summary_new: str,
    raw_new: str,
    date_new: str,
    summary_old: str,
    raw_old: str,
    date_old: str,
) -> bool:
    if summary_new and summary_old and _normalize_text(summary_new) == _normalize_text(summary_old):
        return _same_or_empty(date_new, date_old)
    if raw_new and raw_old and _normalize_text(raw_new) == _normalize_text(raw_old):
        return _same_or_empty(date_new, date_old)
    return False


def _same_or_empty(left: str, right: str) -> bool:
    if not left or not right:
        return True
    return _normalize_text(left) == _normalize_text(right)


def _get_value_by_headers(headers: list[str], row: list[str], names: set[str]) -> str:
    idx = _find_header_index(headers, names)
    if idx is None or idx >= len(row):
        return ""
    return str(row[idx]).strip()


def _format_duplicate_preview(headers: list[str], row: list[str]) -> str:
    date_value = _get_value_by_headers(headers, row, {"–¥–∞—Ç–∞", "–¥–∞—Ç–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è", "–¥–∞—Ç–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è", "date"})
    summary_value = _get_value_by_headers(headers, row, {"—Å—É—Ç—å", "–æ–ø–∏—Å–∞–Ω–∏–µ", "summary", "–Ω–∞ —á—Ç–æ –ø–æ—Ç—Ä–∞—á–µ–Ω–æ"})
    raw_value = _get_value_by_headers(headers, row, {"—Å—ã—Ä–æ–π —Ç–µ–∫—Å—Ç", "raw text", "original text", "–∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç"})

    lines = []
    if date_value:
        lines.append(f"üìÖ –î–∞—Ç–∞: {_shorten(date_value)}")
    if summary_value:
        lines.append(f"üìù –°—É—Ç—å: {_shorten(summary_value)}")
    if raw_value and _normalize_text(raw_value) != _normalize_text(summary_value):
        lines.append(f"üó£Ô∏è –°—ã—Ä–æ–π —Ç–µ–∫—Å—Ç: {_shorten(raw_value, 120)}")
    return "\n".join(lines) if lines else "–ü–æ—Ö–æ–∂–∞—è –∑–∞–ø–∏—Å—å –Ω–∞–π–¥–µ–Ω–∞."


def _shorten(value: str, limit: int = 80) -> str:
    value = value.strip()
    if len(value) <= limit:
        return value
    return value[: limit - 3].rstrip() + "..."


def _find_header_index(headers: list[str], names: set[str]) -> int | None:
    for idx, header in enumerate(headers):
        header_norm = _display_header(header).lower().strip()
        if header_norm in names:
            return idx
    return None


def _normalize_text(value: str) -> str:
    return " ".join(value.lower().split())


def _make_summary(text: str) -> str:
    summary = text.strip()
    if not summary:
        return summary

    prefixes = [
        "—Å–ª—É—à–∞–π",
        "–∞ —Å–ª—É—à–∞–π",
        "–º–Ω–µ –Ω–∞–¥–æ",
        "–º–Ω–µ –Ω—É–∂–Ω–æ",
        "–Ω—É–∂–Ω–æ",
        "—è —Ö–æ—á—É",
        "—Ö–æ—á—É",
        "–º–æ–∂–µ—à—å",
        "–º–æ–∂–µ—à—å –ø–æ–∂–∞–ª—É–π—Å—Ç–∞",
        "–ø–æ–∂–∞–ª—É–π—Å—Ç–∞",
        "–Ω–∞–¥–æ",
    ]
    changed = True
    while changed:
        changed = False
        lowered = summary.lower().lstrip()
        for pref in prefixes:
            if lowered.startswith(pref):
                summary = summary[len(pref):].lstrip(" ,.-")
                changed = True
                break

    suffixes = [
        "–º–æ–∂–µ—à—å –ø–æ—Å—Ç–∞–≤–∏—Ç—å –∑–∞–¥–∞—á–∫—É",
        "–º–æ–∂–µ—à—å –ø–æ—Å—Ç–∞–≤–∏—Ç—å –∑–∞–¥–∞—á—É",
        "–ø–æ—Å—Ç–∞–≤—å –∑–∞–¥–∞—á–∫—É",
        "–ø–æ—Å—Ç–∞–≤—å –∑–∞–¥–∞—á—É",
        "–¥–æ–±–∞–≤—å –≤ –∑–∞–¥–∞—á–∏",
        "–¥–æ–±–∞–≤—å –∑–∞–¥–∞—á—É",
        "–∑–∞–ø–æ–º–Ω–∏ —ç—Ç–æ",
        "–ø–æ–∂–∞–ª—É–π—Å—Ç–∞",
    ]
    lowered = summary.lower()
    for suf in suffixes:
        if lowered.endswith(suf):
            summary = summary[: -len(suf)].rstrip(" ,.-")
            break

    summary = " ".join(summary.split())
    if len(summary) > 160:
        summary = summary[:157].rstrip() + "..."
    return summary or text


def _is_priority_header(header: str) -> bool:
    return "–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç" in header.strip().lower()


def _build_priority_keyboard() -> InlineKeyboardBuilder:
    kb = InlineKeyboardBuilder()
    kb.button(text="–ù–∏–∑–∫–∏–π", callback_data="req:priority:low")
    kb.button(text="–°—Ä–µ–¥–Ω–∏–π", callback_data="req:priority:medium")
    kb.button(text="–í—ã—Å–æ–∫–∏–π", callback_data="req:priority:high")
    kb.button(text="–ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å", callback_data="req:skip")
    kb.button(text="–û—Ç–º–µ–Ω–∞", callback_data="req:cancel")
    kb.adjust(3, 1, 1)
    return kb


def _build_required_keyboard() -> InlineKeyboardBuilder:
    kb = InlineKeyboardBuilder()
    kb.button(text="–ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å", callback_data="req:skip")
    kb.button(text="–û—Ç–º–µ–Ω–∞", callback_data="req:cancel")
    kb.adjust(2)
    return kb


def _build_duplicate_keyboard() -> InlineKeyboardBuilder:
    kb = InlineKeyboardBuilder()
    kb.button(text="‚úÖ –î–æ–±–∞–≤–∏—Ç—å", callback_data="dup:add")
    kb.button(text="‚ùå –ù–µ –¥–æ–±–∞–≤–ª—è—Ç—å", callback_data="dup:skip")
    kb.adjust(2)
    return kb


async def _prompt_category_choice(
    status_msg: Message,
    state: FSMContext,
    categories: list[str],
    transcript: str,
    today_str: str,
) -> None:
    if not categories:
        await status_msg.edit_text("‚ö†Ô∏è –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–∏—Å—Ç Settings.")
        return
    await state.set_state(CategoryState.selecting)
    await state.update_data(
        categories=categories,
        transcript=transcript,
        today_str=today_str,
    )
    kb = _build_category_keyboard(categories)
    await status_msg.edit_text(
        "‚ö†Ô∏è –ù–µ —Å–º–æ–≥ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏—é.\n"
        "–í—ã–±–µ—Ä–∏—Ç–µ –Ω—É–∂–Ω—É—é:",
        reply_markup=kb.as_markup(),
    )


def _build_category_keyboard(categories: list[str]) -> InlineKeyboardBuilder:
    kb = InlineKeyboardBuilder()
    for idx, name in enumerate(categories):
        kb.button(text=name, callback_data=f"cat:pick:{idx}")
    kb.button(text="–û—Ç–º–µ–Ω–∞", callback_data="cat:cancel")
    kb.adjust(2)
    return kb


def _safe_format(template: str, mapping: dict[str, str]) -> str:
    tokens: dict[str, str] = {}
    result = template
    for key, value in mapping.items():
        token = f"__PLACEHOLDER_{key.upper()}__"
        tokens[token] = value
        result = result.replace("{" + key + "}", token)
    result = result.replace("{", "{{").replace("}", "}}")
    for token, value in tokens.items():
        result = result.replace(token, value)
    return result


def _ensure_expected_categories(
    transcript: str,
    settings: dict[str, str],
    items: list[dict[str, str]],
) -> list[dict[str, str]]:
    lowered = transcript.lower()
    result = list(items)

    def has_category(cat_name: str) -> bool:
        return any(item.get("category") == cat_name for item in result)

    task_category = _find_category_by_keywords(settings, ["–∑–∞–¥–∞—á", "task", "todo", "to-do"])
    idea_category = _find_category_by_keywords(settings, ["–∏–¥–µ", "idea"])
    expense_category = _find_category_by_keywords(settings, ["—Ç—Ä–∞—Ç", "—Ä–∞—Å—Ö–æ–¥", "expense", "spend"])

    if task_category and _contains_keywords(
        lowered,
        ["–Ω–∞–¥–æ", "–Ω—É–∂–Ω–æ", "–∑–∞–¥–∞—á", "—Å–¥–µ–ª–∞—Ç—å", "–ø–æ—Å—Ç–∞–≤–∏—Ç—å", "–≤–∏–¥–µ–æ", "—Å–æ–∑–≤–æ–Ω", "–ø–æ–∑–≤–æ–Ω", "–≤—Å—Ç—Ä–µ—Ç"],
    ) and not has_category(task_category):
        text = _extract_sentence(
            transcript,
            ["–Ω–∞–¥–æ", "–Ω—É–∂–Ω–æ", "–∑–∞–¥–∞—á", "—Å–¥–µ–ª–∞—Ç—å", "–ø–æ—Å—Ç–∞–≤–∏—Ç—å", "–≤–∏–¥–µ–æ", "—Å–æ–∑–≤–æ–Ω", "–ø–æ–∑–≤–æ–Ω", "–≤—Å—Ç—Ä–µ—Ç"],
        )
        result.append({"category": task_category, "text": text, "source": "heuristic"})

    if idea_category and _contains_keywords(
        lowered,
        ["–∏–¥–µ", "–∏–¥–µ—è", "idea", "–º–µ—á—Ç–∞", "–ø–ª–∞–Ω", "—Å–æ–∑–¥–∞—Ç—å"],
    ) and not has_category(idea_category):
        text = _extract_sentence(transcript, ["–∏–¥–µ", "–∏–¥–µ—è", "idea", "–º–µ—á—Ç–∞", "–ø–ª–∞–Ω", "—Å–æ–∑–¥–∞—Ç—å"])
        result.append({"category": idea_category, "text": text, "source": "heuristic"})

    if expense_category and _contains_keywords(
        lowered,
        ["–ø–æ—Ç—Ä–∞—Ç", "–∑–∞–ø–ª–∞—Ç–∏–ª", "–∫—É–ø–∏–ª", "—Ä–∞—Å—Ö–æ–¥", "expense", "spend", "—Ä—É–±", "—Ä—É–±–ª", "–¥–æ–ª–ª–∞—Ä", "–º–∞–≥–∞–∑–∏–Ω"],
    ) and not has_category(expense_category):
        text = _extract_sentence(
            transcript,
            ["–ø–æ—Ç—Ä–∞—Ç", "–∑–∞–ø–ª–∞—Ç–∏–ª", "–∫—É–ø–∏–ª", "—Ä–∞—Å—Ö–æ–¥", "expense", "spend", "—Ä—É–±", "—Ä—É–±–ª", "–¥–æ–ª–ª–∞—Ä", "–º–∞–≥–∞–∑–∏–Ω"],
        )
        result.append({"category": expense_category, "text": text, "source": "heuristic"})

    return result


def _find_category_by_keywords(settings: dict[str, str], keywords: list[str]) -> str | None:
    for name in settings.keys():
        lowered = name.strip().lower()
        if any(keyword in lowered for keyword in keywords):
            return name
    return None


def _contains_keywords(text: str, keywords: list[str]) -> bool:
    return any(keyword in text for keyword in keywords)


def _extract_sentence(text: str, keywords: list[str]) -> str:
    parts = re.split(r"[.!?]\s+|\n", text)
    for part in parts:
        lowered = part.lower()
        if any(keyword in lowered for keyword in keywords):
            return part.strip()
    return text.strip()


def _explicit_category_signals(text: str) -> set[str]:
    lowered = text.lower()
    signals: set[str] = set()
    if _contains_keywords(lowered, ["–∑–∞–¥–∞—á", "–∑–∞–¥–∞—á–∞", "—Å–æ–∑–≤–æ–Ω", "–ø–æ–∑–≤–æ–Ω", "–Ω—É–∂–Ω–æ", "–Ω–∞–¥–æ"]):
        signals.add("task")
    if _contains_keywords(lowered, ["–∏–¥–µ", "–∏–¥–µ—è", "–º–µ—á—Ç–∞", "–ø–ª–∞–Ω", "—Å–æ–∑–¥–∞—Ç—å"]):
        signals.add("idea")
    if _contains_keywords(lowered, ["–ø–æ—Ç—Ä–∞—Ç", "–∑–∞–ø–ª–∞—Ç–∏–ª", "–∫—É–ø–∏–ª", "—Ä–∞—Å—Ö–æ–¥", "—Ä—É–±", "–¥–æ–ª–ª–∞—Ä", "–º–∞–≥–∞–∑–∏–Ω"]):
        signals.add("expense")
    return signals


def _coerce_list(value: object) -> list[str]:
    if isinstance(value, list):
        items = [str(item).strip() for item in value]
        return [item for item in items if item]
    if isinstance(value, str) and value.strip():
        return [value.strip()]
    return []


def _format_thinking_blocks(structured: dict) -> str:
    summary = str(structured.get("summary", "")).strip()
    ideas = _coerce_list(structured.get("ideas"))
    tasks = _coerce_list(structured.get("tasks"))
    expenses = _coerce_list(structured.get("expenses"))
    other = _coerce_list(structured.get("other"))

    parts: list[str] = []
    if summary:
        parts.append(f"–ö–æ—Ä–æ—Ç–∫–æ: {summary}")

    sections = [
        ("–ò–¥–µ–∏", "üí°", ideas),
        ("–ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –∑–∞–¥–∞—á–∏", "‚úÖ", tasks),
        ("–¢—Ä–∞—Ç—ã", "üí∏", expenses),
        ("–ü—Ä–æ—á–µ–µ", "üóÇÔ∏è", other),
    ]
    for title, emoji, items in sections:
        if not items:
            continue
        parts.append(f"\n{emoji} {title}")
        parts.extend([f"‚Ä¢ {item}" for item in items])

    return "\n".join(parts).strip()


def _build_thinking_keyboard() -> InlineKeyboardBuilder:
    kb = InlineKeyboardBuilder()
    kb.button(text="‚úÖ –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ ¬´–ü—Ä–æ—á–µ–µ¬ª", callback_data="thinking:save")
    kb.button(text="‚ùå –ù–µ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å", callback_data="thinking:cancel")
    kb.adjust(1)
    return kb


def _build_thinking_inbox_text(structured: dict, transcript: str) -> str:
    blocks = _format_thinking_blocks(structured)
    if blocks:
        return f"{blocks}\n\n–°—ã—Ä–æ–π —Ç–µ–∫—Å—Ç:\n{transcript}".strip()
    return transcript.strip()


async def _handle_thinking_mode(
    status_msg: Message,
    state: FSMContext,
    openai_service: OpenAIService,
    transcript: str,
    today_str: str,
    model: str,
) -> None:
    await status_msg.edit_text("üß† –î–ª–∏–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ. –ü—Ä–∏–≤–æ–∂—É –º—ã—Å–ª–∏ –≤ –ø–æ—Ä—è–¥–æ–∫...")
    user_prompt = _safe_format(DEFAULT_THINKING_USER, {"text": transcript})
    try:
        structured = await openai_service.chat_json(
            system_prompt=DEFAULT_THINKING_SYSTEM,
            user_prompt=user_prompt,
            model=model,
        )
    except Exception:
        logger.exception("Failed to structure long message")
        structured = {
            "summary": _make_summary(transcript),
            "ideas": [],
            "tasks": [],
            "expenses": [],
            "other": [transcript],
        }

    text = _format_thinking_blocks(structured) or transcript
    prompt = (
        f"{text}\n\n"
        "–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ ¬´–ü—Ä–æ—á–µ–µ¬ª?"
    )
    await state.set_state(ThinkingState.waiting_choice)
    await state.update_data(
        thinking_structured=structured,
        thinking_transcript=transcript,
        thinking_today_str=today_str,
    )
    await status_msg.edit_text(prompt, reply_markup=_build_thinking_keyboard().as_markup())


def _rule_based_items_from_transcript(
    transcript: str,
    settings: dict[str, str],
) -> list[dict[str, str]]:
    task_category = _find_category_by_keywords(settings, ["–∑–∞–¥–∞—á", "task", "todo", "to-do"])
    idea_category = _find_category_by_keywords(settings, ["–∏–¥–µ", "idea"])
    expense_category = _find_category_by_keywords(settings, ["—Ç—Ä–∞—Ç", "—Ä–∞—Å—Ö–æ–¥", "expense", "spend"])

    if not any([task_category, idea_category, expense_category]):
        return []

    sentences = _split_sentences(transcript)
    items: list[dict[str, str]] = []
    for sentence in sentences:
        parts = _split_clauses(sentence)
        for part in parts:
            subparts = _split_by_and_if_multi(part)
            for subpart in subparts:
                category = _classify_clause(
                    subpart,
                    task_category,
                    idea_category,
                    expense_category,
                )
                if not category:
                    continue
                if category == task_category:
                    subtasks = _split_task_part(subpart)
                    for sub in subtasks:
                        items.append({"category": category, "text": sub, "source": "rule"})
                else:
                    items.append({"category": category, "text": subpart.strip(), "source": "rule"})

    return items


def _split_sentences(text: str) -> list[str]:
    parts = re.split(r"[.!?]\s+|\n", text)
    return [part.strip() for part in parts if part.strip()]


def _split_clauses(text: str) -> list[str]:
    markers = [
        r"\b—Ç–∞–∫–∂–µ\b",
        r"\b—Ç–∞–∫–∂–µ —è\b",
        r"\b—Ç–∞–∫ –∂–µ\b",
        r"\b–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ\b",
        r"\b–∫—Ä–æ–º–µ —Ç–æ–≥–æ\b",
        r"\b–∞ –µ—â–µ\b",
        r"\b–∏ –µ—â–µ\b",
        r"\b–∏ —É –º–µ–Ω—è\b",
        r"\b–∏ —è\b",
        r"\b–∏ —Ç–∞–∫–∂–µ\b",
        r"\b–ø–ª—é—Å\b",
    ]
    parts = re.split("|".join(markers), text, flags=re.IGNORECASE)
    return [part.strip(" ,.-") for part in parts if part.strip()]


def _split_by_and_if_multi(text: str) -> list[str]:
    signals = _explicit_category_signals(text)
    if len(signals) < 2:
        return [text]
    parts = re.split(r"\s+–∏\s+", text, flags=re.IGNORECASE)
    return [part.strip(" ,.-") for part in parts if part.strip()]


def _classify_clause(
    text: str,
    task_category: str | None,
    idea_category: str | None,
    expense_category: str | None,
) -> str:
    lowered = text.lower()
    scores = []
    if task_category:
        task_score = _keyword_score(
            lowered,
            ["–∑–∞–¥–∞—á", "–∑–∞–¥–∞—á–∞", "–Ω—É–∂–Ω–æ", "–Ω–∞–¥–æ", "—Å–æ–∑–≤–æ–Ω", "–ø–æ–∑–≤–æ–Ω", "—Å–¥–µ–ª–∞—Ç—å", "–≤–∏–¥–µ–æ"],
        )
        scores.append((task_score, task_category))
    if idea_category:
        idea_score = _keyword_score(
            lowered,
            ["–∏–¥–µ", "–∏–¥–µ—è", "–ø–ª–∞–Ω", "—Å–æ–∑–¥–∞—Ç—å", "–∫—É—Ä—Å", "–∫–ª—É–±", "–º–µ—á—Ç–∞"],
        )
        scores.append((idea_score, idea_category))
    if expense_category:
        expense_score = _keyword_score(
            lowered,
            ["–ø–æ—Ç—Ä–∞—Ç", "–∑–∞–ø–ª–∞—Ç–∏–ª", "–∫—É–ø–∏–ª", "—Ä–∞—Å—Ö–æ–¥", "—Ä—É–±", "–¥–æ–ª–ª–∞—Ä", "–º–∞–≥–∞–∑–∏–Ω"],
        )
        scores.append((expense_score, expense_category))

    if not scores:
        return ""
    scores.sort(reverse=True, key=lambda item: item[0])
    if scores[0][0] <= 0:
        return ""
    return scores[0][1]


def _keyword_score(text: str, keywords: list[str]) -> int:
    return sum(1 for keyword in keywords if keyword in text)


def _split_task_part(text: str) -> list[str]:
    if ":" in text:
        _, rest = text.split(":", 1)
        text = rest.strip()
    parts = [part.strip(" ,.-") for part in text.split(" –∏ ") if part.strip()]
    if len(parts) <= 1:
        return [text.strip()]
    return parts


def _expand_item_text(item_text: str, transcript: str, category: str) -> str:
    item_text = item_text.strip() or transcript.strip()
    word_count = len(item_text.split())
    category_lower = (category or "").lower()

    if any(key in category_lower for key in ["—Ç—Ä–∞—Ç", "—Ä–∞—Å—Ö–æ–¥", "expense", "spend"]):
        keywords = ["–ø–æ—Ç—Ä–∞—Ç", "–∫—É–ø–∏–ª", "–∑–∞–ø–ª–∞—Ç–∏–ª", "—Ä—É–±", "–¥–æ–ª–ª–∞—Ä", "–ø–æ–¥–ø–∏—Å–∫", "–º–∞–≥–∞–∑–∏–Ω"]
    elif any(key in category_lower for key in ["–∏–¥–µ", "idea"]):
        keywords = ["–∏–¥–µ", "–∏–¥–µ—è", "—Ö–æ—á—É", "–ø–ª–∞–Ω", "–∫—É—Ä—Å", "–∑–∞–ø—É—Å—Ç–∏—Ç—å", "—Å–æ–∑–¥–∞—Ç—å"]
    elif any(key in category_lower for key in ["–∑–∞–¥–∞—á", "task", "todo"]):
        keywords = ["–∑–∞–¥–∞—á", "–Ω—É–∂–Ω–æ", "–Ω–∞–¥–æ", "—Å–¥–µ–ª–∞—Ç—å", "—Å–æ–∑–≤–æ–Ω", "–ø–æ–∑–≤–æ–Ω", "–≤–∏–¥–µ–æ"]
    else:
        keywords = ["–Ω–∞–¥–æ", "–Ω—É–∂–Ω–æ", "—Ö–æ—á—É", "–ø–æ—Ç—Ä–∞—Ç", "–∏–¥–µ", "–∑–∞–¥–∞—á"]

    expanded = _extract_sentence(transcript, keywords)
    if len(expanded.split()) > word_count:
        return expanded
    return item_text


async def _split_multi_items(
    openai_service: OpenAIService,
    transcript: str,
    settings: dict[str, str],
    model: str,
) -> list[dict[str, str]]:
    rule_items = _rule_based_items_from_transcript(transcript, settings)
    if len(_explicit_category_signals(transcript)) >= 1 and rule_items:
        return rule_items

    categories_text = "\n".join(
        f"- {name}: {desc}" if desc else f"- {name}"
        for name, desc in settings.items()
    )
    user_prompt = _safe_format(
        DEFAULT_MULTI_USER,
        {
            "text": transcript,
            "categories": categories_text,
        },
    )
    try:
        data = await openai_service.chat_json(
            system_prompt=DEFAULT_MULTI_SYSTEM,
            user_prompt=user_prompt,
            model=model,
        )
    except Exception:
        logger.exception("Failed to split multi items")
        return [{"category": "", "text": transcript}]

    items = data.get("items", [])
    if not isinstance(items, list) or not items:
        return [{"category": "", "text": transcript}]

    normalized_map = {name.strip().lower(): name for name in settings.keys()}
    result: list[dict[str, str]] = []
    for item in items:
        category = str(item.get("category", "")).strip()
        text = str(item.get("text", "")).strip() or transcript
        category_norm = category.strip().lower()
        canonical = normalized_map.get(category_norm)
        if not canonical:
            canonical = ""
        result.append({"category": canonical, "text": text})
    return _ensure_expected_categories(transcript, settings, result)


async def _process_multi_items(
    status_msg: Message,
    message: Message,
    state: FSMContext,
    sheets_service: SheetsService,
    router_service: RouterService,
    settings: dict[str, str],
    extract_prompt: str,
    today_str: str,
    items: list[dict[str, str]],
    model: str,
    full_transcript: str,
) -> None:
    results: list[str] = []
    pending: list[dict[str, object]] = []
    today_date = _parse_date_value(today_str) or datetime.now().date()
    for item in items:
        item_text = item.get("text", "")
        category = item.get("category", "")
        if item.get("source") != "rule":
            item_text = _expand_item_text(item_text, full_transcript, category)

        if not category:
            try:
                category, _reasoning = await router_service.classify_category(
                    item_text,
                    settings,
                    DEFAULT_ROUTER_USER,
                    model=model,
                )
            except Exception:
                results.append("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏—é –¥–ª—è –æ–¥–Ω–æ–≥–æ –ø—É–Ω–∫—Ç–∞.")
                continue

        try:
            headers = await sheets_service.get_headers(category)
            if not headers:
                results.append(f"‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω –ª–∏—Å—Ç –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {category}")
                continue
            clean_headers = [_clean_header(header) for header in headers]
            row = await router_service.extract_row(
                item_text,
                clean_headers,
                today_str,
                extract_prompt,
                model=model,
            )
            row = _apply_text_fields(headers, row, item_text)
            row = _apply_date_fields(headers, row, item_text, today_date)

            missing_required = _get_missing_required(headers, row)
            if missing_required:
                pending.append(
                    {
                        "category": category,
                        "headers": headers,
                        "row": row,
                        "transcript": item_text,
                        "today_str": today_str,
                    }
                )
                continue

            duplicate_preview = await _find_duplicate(sheets_service, category, headers, row)
            if duplicate_preview:
                results.append(f"‚ö†Ô∏è –î—É–±–ª–∏–∫–∞—Ç –ø—Ä–æ–ø—É—â–µ–Ω: {category}")
                continue

            await sheets_service.append_row(category, row)
            await sheets_service.append_row("Inbox", [today_str, category, item_text])

            summary = _get_summary_value(headers, row) or _make_summary(item_text)
            results.append(f"‚úÖ {category}: {summary}")
        except Exception:
            logger.exception("Failed to process multi item")
            results.append("‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –æ–¥–Ω–æ–≥–æ –ø—É–Ω–∫—Ç–∞.")

    if pending:
        await state.set_state(IntakeState.waiting_required)
        await state.update_data(
            pending_items=pending,
            pending_index=0,
            multi_results=results,
        )
        await _prompt_required_item(status_msg, state)
        return

    text = "–ì–æ—Ç–æ–≤–æ. –Ø —Ä–∞–∑–æ–±—Ä–∞–ª —Å–æ–æ–±—â–µ–Ω–∏–µ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—É–Ω–∫—Ç–æ–≤:\n\n" + "\n".join(results)
    await _send_long_text(status_msg, message, text, safe_mode=True)


async def _send_long_text(
    status_msg: Message,
    message: Message,
    text: str,
    safe_mode: bool = True,
) -> None:
    chunks = _split_text(text, MAX_TG_CHARS)
    if not chunks:
        return
    if safe_mode and len(chunks) > 3:
        chunks = chunks[:3]
        chunks[-1] = (
            chunks[-1]
            + "\n\n‚Ä¶–°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –£—Ç–æ—á–Ω–∏—Ç–µ –∑–∞–ø—Ä–æ—Å."
        )
    try:
        await status_msg.edit_text(chunks[0])
    except Exception:
        await message.answer(chunks[0])
    for chunk in chunks[1:]:
        await message.answer(chunk)


async def _prompt_required_item(target_msg: Message, state: FSMContext) -> None:
    data = await state.get_data()
    pending = data.get("pending_items", [])
    idx = int(data.get("pending_index", 0))
    if idx < 0 or idx >= len(pending):
        return
    item = pending[idx]
    headers = item.get("headers", [])
    row = item.get("row", [])
    missing_required = _get_missing_required(headers, row)
    await state.update_data(missing_required_indices=[i for i, _name in missing_required])
    if not missing_required:
        return
    if len(missing_required) == 1 and _is_priority_header(missing_required[0][1]):
        kb = _build_priority_keyboard()
        await target_msg.edit_text(
            "‚ö†Ô∏è –ù—É–∂–Ω–æ –≤—ã–±—Ä–∞—Ç—å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –∑–∞–¥–∞—á–∏:",
            reply_markup=kb.as_markup(),
        )
        return
    missing_names = ", ".join(name for _idx, name in missing_required)
    await target_msg.edit_text(
        "‚ö†Ô∏è –ù—É–∂–Ω–æ –∑–∞–ø–æ–ª–Ω–∏—Ç—å –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è:\n"
        f"{missing_names}\n\n"
        "–ù–∞–ø–∏—à–∏—Ç–µ –æ—Ç–≤–µ—Ç —Ç–∞–∫:\n"
        "–ü–æ–ª–µ=–∑–Ω–∞—á–µ–Ω–∏–µ; –ü–æ–ª–µ=–∑–Ω–∞—á–µ–Ω–∏–µ\n"
        "–ü—Ä–∏–º–µ—Ä: –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç=–í—ã—Å–æ–∫–∏–π\n\n"
        "–ú–æ–∂–Ω–æ –Ω–∞–∂–∞—Ç—å ¬´–ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å¬ª.",
        reply_markup=_build_required_keyboard().as_markup(),
    )


def _get_pending_item(data: dict) -> tuple[list[dict], int, dict] | None:
    pending = data.get("pending_items")
    if not pending:
        return None
    idx = int(data.get("pending_index", 0))
    if idx < 0 or idx >= len(pending):
        return None
    return pending, idx, pending[idx]


async def _finalize_multi_item(
    target_msg: Message,
    message: Message,
    state: FSMContext,
    sheets_service: SheetsService,
    item: dict,
    results: list[str],
) -> None:
    category = item.get("category", "")
    headers = item.get("headers", [])
    row = item.get("row", [])
    transcript = item.get("transcript", "")
    today_str = item.get("today_str", datetime.now().strftime("%d.%m.%Y"))
    today_date = _parse_date_value(today_str) or datetime.now().date()

    row = _apply_text_fields(headers, row, transcript)
    row = _apply_date_fields(headers, row, transcript, today_date)

    duplicate_preview = await _find_duplicate(sheets_service, category, headers, row)
    if duplicate_preview:
        results.append(f"‚ö†Ô∏è –î—É–±–ª–∏–∫–∞—Ç –ø—Ä–æ–ø—É—â–µ–Ω: {category}")
    else:
        await sheets_service.append_row(category, row)
        await sheets_service.append_row("Inbox", [today_str, category, transcript])
        summary = _get_summary_value(headers, row) or _make_summary(transcript)
        results.append(f"‚úÖ {category}: {summary}")

    data = await state.get_data()
    pending = data.get("pending_items", [])
    idx = int(data.get("pending_index", 0)) + 1
    if idx < len(pending):
        await state.update_data(pending_index=idx, multi_results=results)
        await _prompt_required_item(target_msg, state)
        return

    await state.clear()
    text = "–ì–æ—Ç–æ–≤–æ. –Ø —Ä–∞–∑–æ–±—Ä–∞–ª —Å–æ–æ–±—â–µ–Ω–∏–µ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—É–Ω–∫—Ç–æ–≤:\n\n" + "\n".join(results)
    await _send_long_text(target_msg, message, text, safe_mode=True)


def _split_text(text: str, max_len: int) -> list[str]:
    text = text.strip()
    if len(text) <= max_len:
        return [text]
    chunks: list[str] = []
    current: list[str] = []
    current_len = 0
    for line in text.splitlines():
        line_len = len(line) + 1
        if line_len > max_len:
            if current:
                chunks.append("\n".join(current).strip())
                current = []
                current_len = 0
            for i in range(0, len(line), max_len):
                chunks.append(line[i : i + max_len])
            continue
        if current_len + line_len > max_len and current:
            chunks.append("\n".join(current).strip())
            current = [line]
            current_len = line_len
        else:
            current.append(line)
            current_len += line_len
    if current:
        chunks.append("\n".join(current).strip())
    return [chunk for chunk in chunks if chunk]
